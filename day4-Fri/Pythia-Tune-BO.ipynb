{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial PYTHIA Tuning\n",
    "\n",
    "This tutorial demostrates adversarial optimization on non-differentiable generators.\n",
    "\n",
    "We mostly follow https://arxiv.org/abs/1610.08328 (*TuneMC*) except for:\n",
    "- only one parameter is optimized;\n",
    "- adversarial objective instead of $\\chi^2$.\n",
    "\n",
    "**Note: this notebook takes quite a long time to execute. It is recommended to run all cells at the beginning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "Pythia-mill is a python binding to Pythia generator that can run in multiple threads (processes).\n",
    "For more details, please, visit https://github.com/maxim-borisyak/pythia-mill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pythiamill as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, speed and clarity only one of Pythia parameters is tuned: `alphaSvalue`.\n",
    "\n",
    "The rest are fixed according to Monash tune (https://arxiv.org/abs/1404.5630)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_options = [\n",
    "    ### telling pythia to be quiet.\n",
    "    'Print:quiet = on',\n",
    "    'Init:showProcesses = off',\n",
    "    'Init:showMultipartonInteractions = off',\n",
    "    'Init:showChangedSettings = off',\n",
    "    'Init:showChangedParticleData = off',\n",
    "    'Next:numberCount=0',\n",
    "    'Next:numberShowInfo=0',\n",
    "    'Next:numberShowEvent=0',\n",
    "    'Stat:showProcessLevel=off',\n",
    "    'Stat:showErrors=off',\n",
    "    \n",
    "    ### seeting default parameters to Monash values\n",
    "    \"Tune:ee = 7\",\n",
    "    \"Beams:idA = 11\",\n",
    "    \"Beams:idB = -11\",\n",
    "    \"Beams:eCM = 91.2\",\n",
    "    \"WeakSingleBoson:ffbar2gmZ = on\",\n",
    "    \"23:onMode = off\",\n",
    "    \"23:onIfMatch = 1 -1\",\n",
    "    \"23:onIfMatch = 2 -2\",\n",
    "    \"23:onIfMatch = 3 -3\",\n",
    "    \"23:onIfMatch = 4 -4\",\n",
    "    \"23:onIfMatch = 5 -5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_samples=2**14, alphaSvalue=0.1365, n_workers=4, seed=123, show_progress=False):\n",
    "    options = fixed_options + [ 'TimeShower:alphaSvalue=%f' % alphaSvalue ]\n",
    "    \n",
    "    ### TuneMC detector provides the same features used in TuneMC paper\n",
    "    detector = pm.utils.TuneMCDetector()\n",
    "    \n",
    "    mill = pm.PythiaMill(detector, options, batch_size=256, cache_size=4, n_workers=n_workers, seed=seed)\n",
    "    \n",
    "    ### sampling\n",
    "    data = np.vstack([\n",
    "        mill.sample()\n",
    "        for _ in (\n",
    "            lambda x: tqdm_notebook(x, postfix='data gen')\n",
    "            if show_progress else\n",
    "            lambda x: x\n",
    "        )(range(n_samples // 256))\n",
    "    ])\n",
    "    \n",
    "    mill.terminate()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_train = get_data(show_progress=True)\n",
    "X_true_val = get_data(show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.activations import softplus, sigmoid, relu\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### don't forget about others!\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.1)\n",
    "\n",
    "tf_session = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "    gpu_options=gpu_options,\n",
    "\n",
    "    ### uncomment to use cpu\n",
    "    ###device_count = {'GPU': 0}\n",
    "))\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(tf_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = lambda x: relu(x, 0.05)\n",
    "\n",
    "inputs = Input(shape=(445, ))\n",
    "\n",
    "net = Dense(128, activation=activation)(inputs)\n",
    "predictions = Dense(1, activation=sigmoid)(net)\n",
    "\n",
    "discriminator = Model(inputs=inputs, outputs=predictions)\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.display import SVG\n",
    "\n",
    "SVG(model_to_dot(discriminator, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = discriminator.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def jensen_shannon(alphaSvalue, n_epoches=5):\n",
    "    discriminator.set_weights(initial_weights)\n",
    "    \n",
    "    X_gen_train = get_data(alphaSvalue=alphaSvalue, show_progress=True)\n",
    "    X_gen_val = get_data(alphaSvalue=alphaSvalue, show_progress=True)\n",
    "    \n",
    "    X_train = np.vstack([ X_gen_train, X_true_train ])\n",
    "    y_train = np.hstack([ np.zeros(X_gen_train.shape[0]), np.ones(X_true_train.shape[0]) ]).astype('float32')\n",
    "    \n",
    "    history = discriminator.fit(x=X_train, y=y_train, batch_size=32, epochs=n_epoches, verbose=0)\n",
    "    \n",
    "    X_val = np.vstack([X_gen_val, X_true_val])\n",
    "    y_val = np.hstack([ np.zeros(X_gen_val.shape[0]), np.ones(X_true_val.shape[0]) ]).astype('float32')\n",
    "    \n",
    "    proba = discriminator.predict(X_val)\n",
    "    \n",
    "    return np.log(2) - log_loss(y_val, proba, eps=1.0e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed-up computations we pretrain network on some possible configuration.\n",
    "\n",
    "The main idea behind this: the difference between optimal networks for some configurations of Pythia is much easier to learn than between optimal network for some configuration and a random initialization - essentially, with pretraining, network only needs to learn small corrections instead of learning everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "jensen_shannon(0.25, n_epoches=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = discriminator.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.acquisition import gaussian_ei as acq_func\n",
    "\n",
    "def plot_bo(bo, suggestion=None, value=None):\n",
    "    a, b = bo.space.bounds[0]\n",
    "    \n",
    "    ### getting the latest model\n",
    "    model = bo.models[-1]\n",
    "    \n",
    "    xs = np.linspace(a, b, num=100)\n",
    "    x_model = bo.space.transform(xs.reshape(-1, 1).tolist())\n",
    "    \n",
    "    mean, std = model.predict(x_model, return_std=True)\n",
    "    \n",
    "    plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(\n",
    "        np.array(bo.Xi)[:, 0],\n",
    "        np.array(bo.yi),\n",
    "        color='red',\n",
    "        label='observations'\n",
    "    )\n",
    "    if suggestion is not None:\n",
    "        plt.scatter([suggestion], value, color='blue', label='suggestion')\n",
    "    \n",
    "    plt.plot(xs, mean, color='green', label='model')\n",
    "    plt.fill_between(xs, mean - 1.96 * std, mean + 1.96 * std, alpha=0.1, color='green')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    acq = acq_func(x_model, model, np.min(bo.yi))\n",
    "    plt.plot(xs, acq, label='Expected Improvement')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = Optimizer(\n",
    "    dimensions=[\n",
    "        (0.06, 0.25)\n",
    "    ],\n",
    "    base_estimator='gp',\n",
    "    n_initial_points=3,\n",
    "    acq_func='EI',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = bo.ask()\n",
    "    f = jensen_shannon(x[0])\n",
    "\n",
    "    if len(bo.models) > 0:\n",
    "        plot_bo(bo, suggestion=x, value=f)\n",
    "    \n",
    "    bo.tell(x, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bo.models[-1]\n",
    "space = bo.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0.06, 0.25, num=100)\n",
    "predicted_mean, predicted_std = model.predict(space.transform(xs.reshape(-1, 1)), return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(xs, predicted_mean, color='green', label='estimated mean value')\n",
    "plt.fill_between(\n",
    "    xs, predicted_mean - 1.96 * predicted_std, predicted_mean + 1.96 * predicted_std,\n",
    "    color='green', alpha=0.2,\n",
    "    label='estimated uncertainty'\n",
    ")\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is next?\n",
    "\n",
    "In most cases, the point minimizing estimated mean is a reasonable choice. However, depending on a particular problem, you might want to use a different criteria, for example, introduce a trade-off between mean value and uncertanty (e.g. similar to Lower Confedence Bound acqusition function) or continue optimization untilcertain guarantees are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "**Task 1:**\n",
    "\n",
    "Below you can find a set of 20 parameters (also *monash* values, i.e. ground-truth and some reasonable ranges) used in https://arxiv.org/abs/1610.08328 (*TuneMC*).\n",
    "\n",
    "Pick several of them (e.g. first 3) and try to converge on ground-truth.\n",
    "\n",
    "**Task 2:**\n",
    "\n",
    "Instead of a good discriminator, try to use a really weak one (e.g. XGboost with 2 trees of depth 3), see what happens.\n",
    "Compare resulting model to one obtained with a strong discriminator.  \n",
    "\n",
    "Better to do it on 1D tune.\n",
    "\n",
    "Can we use it to our advantage?\n",
    "\n",
    "**Tips:**\n",
    "- XGBoost works fine with this data (and considerably faster);\n",
    "- don't forget to check for overfitting (even though I did not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names20 = [\n",
    "  \"TimeShower:alphaSvalue\",\n",
    "  \"TimeShower:pTmin\",\n",
    "  \"TimeShower:pTminChgQ\",\n",
    "\n",
    "  \"StringPT:sigma\",\n",
    "  \"StringZ:bLund\",\n",
    "  \"StringZ:aExtraSQuark\",\n",
    "  \"StringZ:aExtraDiquark\",\n",
    "  \"StringZ:rFactC\",\n",
    "  \"StringZ:rFactB\",\n",
    "\n",
    "  \"StringFlav:probStoUD\",\n",
    "  \"StringFlav:probQQtoQ\",\n",
    "  \"StringFlav:probSQtoQQ\",\n",
    "  \"StringFlav:probQQ1toQQ0\",\n",
    "  \"StringFlav:mesonUDvector\",\n",
    "  \"StringFlav:mesonSvector\",\n",
    "  \"StringFlav:mesonCvector\",\n",
    "  \"StringFlav:mesonBvector\",\n",
    "  \"StringFlav:etaSup\",\n",
    "  \"StringFlav:etaPrimeSup\",\n",
    "  \"StringFlav:decupletSup\"\n",
    "]\n",
    "\n",
    "space20 = np.array([\n",
    "  (0.06, 0.25),\n",
    "  (0.1, 2.0),\n",
    "  (0.1, 2.0),\n",
    "\n",
    "  (0.2, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 2.0),\n",
    "  (0.0, 2.0),\n",
    "  (0.0, 2.0),\n",
    "  (0.0, 2.0),\n",
    "\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 1.0),\n",
    "  (0.0, 3.0),\n",
    "  (0.0, 3.0),\n",
    "  (0.0, 3.0),\n",
    "  (0.0, 3.0)\n",
    "])\n",
    "\n",
    "monash20 = np.array([\n",
    "  0.1365,\n",
    "  0.5,\n",
    "  0.5,\n",
    "\n",
    "  0.98,\n",
    "  0.335,\n",
    "  0,\n",
    "  0.97,\n",
    "  1.32,\n",
    "  0.885,\n",
    "\n",
    "  0.217,\n",
    "  0.081,\n",
    "  0.915,\n",
    "  0.0275,\n",
    "  0.6,\n",
    "  0.12,\n",
    "  1,\n",
    "  0.5,\n",
    "  0.55,\n",
    "  0.88,\n",
    "  2.2\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
